# ğŸ§  Dr. X Research Q&A System â€” OSOS AI Technical Test

This repository contains my solution to the OSOS AI NLP Technical Challenge. The task was to analyze Dr. Xâ€™s multi-format publications and build a complete RAG-based Q&A system using only **local models** and **offline vector databases**.

---

## ğŸ“¦ Project Structure
DrX_Research_QA/ 
â”‚â”€â”€ data/   # Store Dr. X's publications 
â”œâ”€â”€ chunks/ # Chunked text with metadata 
â”œâ”€â”€ extracted/ # Raw extracted text files 
â”‚
â”œâ”€â”€ models/                   # For local LLMs or embedding models
â”œâ”€â”€ vectorstore/ # FAISS index + metadata 
â”‚
â”œâ”€â”€ translated/ # Translated chunks 
â”œâ”€â”€ summaries/ # Summarized chunks 
â”‚ 
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_extract_text.py     # File extraction from PDFs, DOCX, XLSX, CSV
â”‚   â”œâ”€â”€ 2_chunk_text.py       # Tokenize and chunk using cl100k_base
â”‚   â”œâ”€â”€ 3_embed_and_store.py  # Embedding + vector DB creation
â”‚   â”œâ”€â”€ 4_rag_qa.py           # RAG Q&A system with local LLaMA
â”‚   â”œâ”€â”€ 5_translate.py        # Translation tool
â”‚   â”œâ”€â”€ 6_summarize.py        # Summarization + ROUGE
â”‚   â””â”€â”€ utils.py              # Utility functions
â”‚
â”œâ”€â”€ performance.log 
â”œâ”€â”€ README.md 
â””â”€â”€ requirements.txt



---

## âœ… Features

- ğŸ“„ Multi-format file processing: PDF, DOCX, CSV, XLSX, XLSM
- ğŸ§© Token-based chunking with `cl100k_base` tokenizer
- ğŸ§  Vector embeddings using `nomic-ai/nomic-embed-text-v1`
- ğŸ§  Local RAG Q&A using `llama-cpp-python` with LLaMA GGUF model
- ğŸŒ Multilingual translation to English/Arabic using `facebook/nllb-200-distilled-600M`
- âœ‚ï¸ Summarization using `facebook/bart-large-cnn`
- ğŸ“Š ROUGE-1 and ROUGE-L evaluation
- ğŸš€ Tokens/sec logging for all major steps

---

## ğŸ§ª Performance Logging

All performance is logged to `performance.log`:
- Total tokens processed
- Time taken
- Tokens per second
- Per-file and per-query level

---

## ğŸ” Local Models Used

| Task            | Model                                    |
|-----------------|-------------------------------------------|
| Embedding       | `nomic-ai/nomic-embed-text-v1`           |
| Translation     | `facebook/nllb-200-distilled-600M`       |
| Summarization   | `facebook/bart-large-cnn`                |
| RAG LLM         | `llama-2-7b.Q4_K_M.gguf` (via llama.cpp) |

---

## ğŸ› ï¸ How to Run

1. Install dependencies:
    ```bash
    pip install -r requirements.txt

2. Put your .pdf, .docx, .xlsx, etc. into the data/ folder

3. Run each step:
    python 1_extract_text.py
    python 2_chunk_text.py
    python 3_embed_and_store.py
    python 4_rag_qa.py
    python 5_translate.py
    python 6_summarize.py

## ğŸ“œ Requirements

faiss-cpu
sentence-transformers
transformers
langdetect
tiktoken
llama-cpp-python
rouge-score
evaluate
pandas
python-docx
openpyxl
tqdm


## ğŸ§  Notes

*   All models run locally with no external APIs or internet required.
*   No vision-based tools were used for PDFs â€” purely NLP-based extraction.
*   Vector DB is built using FAISS for offline efficiency.
